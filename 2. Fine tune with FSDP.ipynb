{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train Starcoder model with multi-node clusters on Amazon SageMaker using Hugging Face and PyTorch FSDP\n",
    "\n",
    "In this tutorial, we will fine-tune the new [LLama2-7B](https://huggingface.co/meta-llama/Llama-2-7b) on the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset to improve the question-answering skills.\n",
    "\n",
    "[LLama2-7B](https://huggingface.co/meta-llama/Llama-2-7b) is a 7B open-source LLM, which makes it hard to fine-tune on a single GPU or even a single Node with multiple GPUs. We are going to use Amazon SageMaker managed training platform as our infrastructure backbone to help us create a multi-node cluster to easily run our distributed training. As instances, we will use 2x p4d.24xlarge instances, which come with 8x NIVIDA A100 40GB GPUs. \n",
    "\n",
    "*Note: For the purpose of this workshop we will use a smaller 3 billion Parameter model and will use G5.12xlarge instance which comes with 4X Nvidia A10G 24GB GPUs..*\n",
    "\n",
    "As distributed training framework, we will use Pytorch FSDP + Hugging Face Transformers Trainer, which will make it super easy to distribute our model and data in a fully sharded way across all our nodes and GPUs.\n",
    "\n",
    "\n",
    "## What is PyTorch Fully Sharded Data Parallel (FSDP)?\n",
    "\n",
    "PyTorch FSDP (Fully Sharded Data Parallel) is an extension of data parallelism that enables efficient large-scale training of LLMs. With FSDP, each GPU stores only a subset of the model and associated optimizer states and gradients and can optionally offload the sharded model parameters to CPUs. This helps maximize the overlap between network communication and model computation, reducing the memory footprint on GPUs.\n",
    "\n",
    "FSDP optimizations include:\n",
    "\n",
    "- Transformer Wrapping Policy\n",
    "- Mixed Precision (bf16)\n",
    "- Activation Checkpointing (Gradient Checkpointing)\n",
    "- Full Sharding Strategy\n",
    "\n",
    "PyTorch FSDP is natively integrated into the [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#pytorch-fully-sharded-data-parallel), making it easy to adapt and use. You can learn more about PyTorch FSDP in [Efficient Large-Scale Training with Pytorch FSDP and AWS](https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/) or [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "awscli 1.29.41 requires botocore==1.31.41, but you have botocore 1.31.45 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers\" \"datasets[s3]\" \"sagemaker\" \"boto3\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31 (from -r scripts/requirements.txt (line 1))\n",
      "  Obtaining dependency information for transformers==4.31 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r scripts/requirements.txt (line 2)) (2.14.5)\n",
      "Collecting accelerate>=0.21 (from -r scripts/requirements.txt (line 3))\n",
      "  Obtaining dependency information for accelerate>=0.21 from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting bitsandbytes (from -r scripts/requirements.txt (line 4))\n",
      "  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/1e/2c/af22cd797fc368a9f098ed03015730e6568b884fe67f9940793d944a4b7b/bitsandbytes-0.41.1-py3-none-any.whl.metadata\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting peft (from -r scripts/requirements.txt (line 5))\n",
      "  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31->-r scripts/requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->-r scripts/requirements.txt (line 2)) (3.8.5)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.21->-r scripts/requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.21->-r scripts/requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->-r scripts/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31->-r scripts/requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31->-r scripts/requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.31->-r scripts/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.31->-r scripts/requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.31->-r scripts/requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r scripts/requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r scripts/requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r scripts/requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->-r scripts/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->-r scripts/requirements.txt (line 2)) (2023.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->-r scripts/requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.21->-r scripts/requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.21->-r scripts/requirements.txt (line 3)) (1.3.0)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, transformers, accelerate, peft\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.33.1\n",
      "    Uninstalling transformers-4.33.1:\n",
      "      Successfully uninstalled transformers-4.33.1\n",
      "Successfully installed accelerate-0.22.0 bitsandbytes-0.41.1 peft-0.5.0 transformers-4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::365792799466:role/test_step_role\n",
      "sagemaker bucket: sagemaker-us-west-2-365792799466\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset, but before fine-tuning the model, we need to preprocess the data. We will create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to load our dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\"\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "dataset_name = \"tatsu-lab/alpaca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a70bbc8400b4f44badfbf64a3b102d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ffc0cb46b84433a2e8e3847aa3160a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5797a65130e94b65b70e1ff6a35ed6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122c6c7f1f524e13bad11a914cc77dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e11a23f61e34d38acd763804c212ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc34ffbd72d640f9b47503d44d3415c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6157bb29794cc7b04e000e2369ac25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d869e34c1d864d9c8f37bda10ec414c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14cb58d124c4c8aa591914f48694a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "from huggingface_hub.hf_api import HfFolder;\n",
    "HfFolder.save_token(access_token)\n",
    "\n",
    "# Load Tokenizer \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,token=access_token)\n",
    "\n",
    "# Load dataset from huggingface.co\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# downsample dataset to 10k\n",
    "dataset = dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into Train and Valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in dataset.keys():\n",
    "    dataset[\"validation\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=\"train[:5%]\"\n",
    "    )\n",
    "\n",
    "    dataset[\"train\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=\"train[5%:]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset contains 4 fields instruction, input , output and text. The text field is obtained by combining the remaining 3 fields and we will use the text field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the data preparation is to tokenize and chunk our dataset. We convert our inputs (text) to token IDs by tokenizing, which the model can understand. Additionally, we concatenate our dataset samples into chunks of `2048` to avoid unnecessary padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a173bb368a4b4b14b5cc067878503782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d499ad1895ae4e84ae36fb9fb56a573f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae48fb1d1580453b87efd873f122ba04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e92b282912b4db4a6021326d1f507f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "def group_texts(examples,block_size = 2048):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"],return_token_type_ids=False), batched=True, remove_columns=list(column_names)\n",
    ").map(\n",
    "    partial(group_texts, block_size=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama v2 7b model using FSDP locally. \n",
    "\n",
    "We will use the 4 GPU's available in this notebook instance to launch a distributed training job using torch distributed(torchrun). \n",
    "\n",
    "We will start by saving the tokenized data locally ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef450a43c554d13ad04f30192027b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165f01916d0d48beb69afc5e545ae83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to: processed/data/\n"
     ]
    }
   ],
   "source": [
    "#save data locally\n",
    "\n",
    "training_input_path = f'processed/data/'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"Saved data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 614/614 [00:00<00:00, 6.69MB/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██| 26.8k/26.8k [00:00<00:00, 198MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   0%|  | 21.0M/9.98G [00:00<00:50, 198MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|  | 62.9M/9.98G [00:00<00:32, 306MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|   | 105M/9.98G [00:00<00:28, 345MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|   | 147M/9.98G [00:00<00:27, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   2%|   | 189M/9.98G [00:00<00:26, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   2%|   | 231M/9.98G [00:00<00:25, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|   | 273M/9.98G [00:00<00:24, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|   | 315M/9.98G [00:00<00:24, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|   | 357M/9.98G [00:00<00:24, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|   | 398M/9.98G [00:01<00:24, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▏  | 440M/9.98G [00:01<00:24, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▏  | 482M/9.98G [00:01<00:24, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▏  | 524M/9.98G [00:01<00:24, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▏  | 566M/9.98G [00:01<00:24, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▏  | 608M/9.98G [00:01<00:24, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▏  | 650M/9.98G [00:01<00:23, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▏  | 692M/9.98G [00:01<00:23, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▏  | 734M/9.98G [00:01<00:23, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▏  | 776M/9.98G [00:02<00:23, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▏  | 818M/9.98G [00:02<00:23, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▎  | 860M/9.98G [00:02<00:23, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▎  | 902M/9.98G [00:02<00:23, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▎  | 944M/9.98G [00:02<00:23, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  10%|▎  | 986M/9.98G [00:02<00:22, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  10%|▏ | 1.03G/9.98G [00:02<00:22, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  11%|▏ | 1.07G/9.98G [00:02<00:22, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  11%|▏ | 1.11G/9.98G [00:02<00:22, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|▏ | 1.15G/9.98G [00:03<00:22, 395MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|▏ | 1.20G/9.98G [00:03<00:22, 396MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|▏ | 1.24G/9.98G [00:03<00:22, 395MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  13%|▎ | 1.28G/9.98G [00:03<00:22, 395MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  13%|▎ | 1.32G/9.98G [00:03<00:21, 394MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  14%|▎ | 1.36G/9.98G [00:03<00:21, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  14%|▎ | 1.41G/9.98G [00:03<00:21, 394MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|▎ | 1.45G/9.98G [00:03<00:22, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|▎ | 1.49G/9.98G [00:03<00:21, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|▎ | 1.53G/9.98G [00:03<00:21, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|▎ | 1.57G/9.98G [00:04<00:21, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|▎ | 1.61G/9.98G [00:04<00:21, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|▎ | 1.66G/9.98G [00:04<00:21, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|▎ | 1.70G/9.98G [00:04<00:21, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|▎ | 1.74G/9.98G [00:04<00:21, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  18%|▎ | 1.78G/9.98G [00:04<00:21, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  18%|▎ | 1.82G/9.98G [00:04<00:20, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|▎ | 1.87G/9.98G [00:04<00:20, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|▍ | 1.91G/9.98G [00:04<00:20, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  20%|▍ | 1.95G/9.98G [00:05<00:24, 325MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  20%|▍ | 1.99G/9.98G [00:05<00:23, 344MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  20%|▍ | 2.03G/9.98G [00:05<00:22, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  21%|▍ | 2.08G/9.98G [00:05<00:21, 364MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  21%|▍ | 2.12G/9.98G [00:05<00:21, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|▍ | 2.16G/9.98G [00:05<00:20, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|▍ | 2.20G/9.98G [00:05<00:20, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|▍ | 2.24G/9.98G [00:05<00:19, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  23%|▍ | 2.29G/9.98G [00:05<00:19, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  23%|▍ | 2.33G/9.98G [00:06<00:19, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  24%|▍ | 2.37G/9.98G [00:06<00:19, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  24%|▍ | 2.41G/9.98G [00:06<00:19, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|▍ | 2.45G/9.98G [00:06<00:19, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|▌ | 2.50G/9.98G [00:06<00:19, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|▌ | 2.54G/9.98G [00:06<00:19, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  26%|▌ | 2.58G/9.98G [00:06<00:19, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  26%|▌ | 2.62G/9.98G [00:06<00:19, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  27%|▌ | 2.66G/9.98G [00:06<00:18, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  27%|▌ | 2.71G/9.98G [00:07<00:18, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|▌ | 2.75G/9.98G [00:07<00:18, 386MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|▌ | 2.79G/9.98G [00:07<00:18, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|▌ | 2.83G/9.98G [00:07<00:18, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  29%|▌ | 2.87G/9.98G [00:07<00:18, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  29%|▌ | 2.92G/9.98G [00:07<00:18, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|▌ | 2.96G/9.98G [00:07<00:17, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|▌ | 3.00G/9.98G [00:07<00:17, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|▌ | 3.04G/9.98G [00:07<00:17, 395MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  31%|▌ | 3.08G/9.98G [00:08<00:17, 394MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  31%|▋ | 3.12G/9.98G [00:08<00:17, 394MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  32%|▋ | 3.17G/9.98G [00:08<00:17, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  32%|▋ | 3.21G/9.98G [00:08<00:17, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|▋ | 3.25G/9.98G [00:08<00:17, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|▋ | 3.29G/9.98G [00:08<00:17, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|▋ | 3.33G/9.98G [00:08<00:16, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|▋ | 3.38G/9.98G [00:08<00:16, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|▋ | 3.42G/9.98G [00:08<00:17, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|▋ | 3.46G/9.98G [00:09<00:17, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|▋ | 3.50G/9.98G [00:09<00:17, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  36%|▋ | 3.54G/9.98G [00:09<00:17, 375MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors:  36%|▋ | 3.59G/9.98G [00:09<00:16, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  36%|▋ | 3.63G/9.98G [00:09<00:16, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  37%|▋ | 3.67G/9.98G [00:09<00:16, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  37%|▋ | 3.71G/9.98G [00:09<00:16, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  38%|▊ | 3.75G/9.98G [00:09<00:16, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  38%|▊ | 3.80G/9.98G [00:09<00:16, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  38%|▊ | 3.84G/9.98G [00:09<00:15, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  39%|▊ | 3.88G/9.98G [00:10<00:18, 329MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  39%|▊ | 3.92G/9.98G [00:10<00:18, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  40%|▊ | 3.96G/9.98G [00:10<00:17, 346MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  40%|▊ | 4.01G/9.98G [00:10<00:16, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  41%|▊ | 4.05G/9.98G [00:10<00:16, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  41%|▊ | 4.09G/9.98G [00:10<00:16, 353MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  41%|▊ | 4.13G/9.98G [00:10<00:16, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  42%|▊ | 4.17G/9.98G [00:10<00:16, 348MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  42%|▊ | 4.22G/9.98G [00:11<00:16, 347MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  43%|▊ | 4.26G/9.98G [00:11<00:16, 352MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  43%|▊ | 4.30G/9.98G [00:11<00:15, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  44%|▊ | 4.34G/9.98G [00:11<00:15, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  44%|▉ | 4.38G/9.98G [00:11<00:15, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  44%|▉ | 4.42G/9.98G [00:11<00:14, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  45%|▉ | 4.47G/9.98G [00:11<00:14, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  45%|▉ | 4.51G/9.98G [00:11<00:14, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|▉ | 4.55G/9.98G [00:11<00:14, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|▉ | 4.59G/9.98G [00:12<00:13, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|▉ | 4.63G/9.98G [00:12<00:13, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  47%|▉ | 4.68G/9.98G [00:12<00:13, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  47%|▉ | 4.72G/9.98G [00:12<00:13, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  48%|▉ | 4.76G/9.98G [00:12<00:13, 385MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  48%|▉ | 4.80G/9.98G [00:12<00:13, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|▉ | 4.84G/9.98G [00:12<00:13, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|▉ | 4.89G/9.98G [00:12<00:13, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|▉ | 4.93G/9.98G [00:12<00:12, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  50%|▉ | 4.97G/9.98G [00:13<00:12, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  50%|█ | 5.01G/9.98G [00:13<00:12, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█ | 5.05G/9.98G [00:13<00:12, 389MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█ | 5.10G/9.98G [00:13<00:12, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█ | 5.14G/9.98G [00:13<00:12, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█ | 5.18G/9.98G [00:13<00:12, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█ | 5.22G/9.98G [00:13<00:19, 243MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█ | 5.26G/9.98G [00:14<00:17, 276MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█ | 5.31G/9.98G [00:14<00:16, 291MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█ | 5.35G/9.98G [00:14<00:14, 316MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█ | 5.39G/9.98G [00:14<00:13, 335MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█ | 5.43G/9.98G [00:14<00:13, 348MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█ | 5.47G/9.98G [00:14<00:12, 351MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█ | 5.52G/9.98G [00:14<00:12, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█ | 5.56G/9.98G [00:14<00:12, 358MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█ | 5.60G/9.98G [00:14<00:12, 358MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█▏| 5.64G/9.98G [00:15<00:12, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█▏| 5.68G/9.98G [00:15<00:11, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█▏| 5.73G/9.98G [00:15<00:11, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█▏| 5.77G/9.98G [00:15<00:11, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█▏| 5.81G/9.98G [00:15<00:11, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█▏| 5.85G/9.98G [00:15<00:11, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█▏| 5.89G/9.98G [00:15<00:11, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█▏| 5.93G/9.98G [00:15<00:11, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█▏| 5.98G/9.98G [00:16<00:11, 356MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█▏| 6.02G/9.98G [00:16<00:11, 353MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  61%|█▏| 6.06G/9.98G [00:16<00:11, 355MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  61%|█▏| 6.10G/9.98G [00:16<00:10, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  62%|█▏| 6.14G/9.98G [00:16<00:10, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  62%|█▏| 6.19G/9.98G [00:16<00:10, 363MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  62%|█▏| 6.23G/9.98G [00:16<00:10, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  63%|█▎| 6.27G/9.98G [00:16<00:10, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  63%|█▎| 6.31G/9.98G [00:16<00:09, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|█▎| 6.35G/9.98G [00:17<00:09, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|█▎| 6.40G/9.98G [00:17<00:09, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|█▎| 6.44G/9.98G [00:17<00:09, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|█▎| 6.48G/9.98G [00:17<00:09, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|█▎| 6.52G/9.98G [00:17<00:09, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  66%|█▎| 6.56G/9.98G [00:17<00:09, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  66%|█▎| 6.61G/9.98G [00:17<00:09, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|█▎| 6.65G/9.98G [00:17<00:08, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|█▎| 6.69G/9.98G [00:17<00:08, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|█▎| 6.73G/9.98G [00:18<00:08, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  68%|█▎| 6.77G/9.98G [00:18<00:08, 369MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  68%|█▎| 6.82G/9.98G [00:18<00:08, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|█▎| 6.86G/9.98G [00:18<00:08, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|█▍| 6.90G/9.98G [00:18<00:08, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|█▍| 6.94G/9.98G [00:18<00:08, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|█▍| 6.98G/9.98G [00:18<00:08, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|█▍| 7.03G/9.98G [00:18<00:07, 369MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|█▍| 7.07G/9.98G [00:18<00:07, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|█▍| 7.11G/9.98G [00:19<00:07, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  72%|█▍| 7.15G/9.98G [00:19<00:07, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  72%|█▍| 7.19G/9.98G [00:19<00:07, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|█▍| 7.24G/9.98G [00:19<00:07, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|█▍| 7.28G/9.98G [00:19<00:07, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|█▍| 7.32G/9.98G [00:19<00:07, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  74%|█▍| 7.36G/9.98G [00:19<00:06, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  74%|█▍| 7.40G/9.98G [00:19<00:06, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|█▍| 7.44G/9.98G [00:19<00:06, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|█▌| 7.49G/9.98G [00:20<00:06, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|█▌| 7.53G/9.98G [00:20<00:06, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  76%|█▌| 7.57G/9.98G [00:20<00:06, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  76%|█▌| 7.61G/9.98G [00:20<00:06, 375MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors:  77%|█▌| 7.65G/9.98G [00:20<00:06, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  77%|█▌| 7.70G/9.98G [00:20<00:06, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|█▌| 7.74G/9.98G [00:20<00:05, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|█▌| 7.78G/9.98G [00:20<00:05, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|█▌| 7.82G/9.98G [00:20<00:05, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|█▌| 7.86G/9.98G [00:21<00:05, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|█▌| 7.91G/9.98G [00:21<00:06, 306MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  80%|█▌| 7.95G/9.98G [00:21<00:06, 319MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  80%|█▌| 7.99G/9.98G [00:21<00:05, 332MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|█▌| 8.03G/9.98G [00:21<00:05, 345MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|█▌| 8.07G/9.98G [00:21<00:05, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|█▋| 8.12G/9.98G [00:21<00:05, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  82%|█▋| 8.16G/9.98G [00:21<00:04, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  82%|█▋| 8.20G/9.98G [00:22<00:04, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|█▋| 8.24G/9.98G [00:22<00:04, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|█▋| 8.28G/9.98G [00:22<00:04, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|█▋| 8.33G/9.98G [00:22<00:04, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  84%|█▋| 8.37G/9.98G [00:22<00:04, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  84%|█▋| 8.41G/9.98G [00:22<00:04, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  85%|█▋| 8.45G/9.98G [00:22<00:04, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  85%|█▋| 8.49G/9.98G [00:22<00:03, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  86%|█▋| 8.54G/9.98G [00:22<00:03, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  86%|█▋| 8.58G/9.98G [00:23<00:03, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  86%|█▋| 8.62G/9.98G [00:23<00:03, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|█▋| 8.66G/9.98G [00:23<00:03, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|█▋| 8.70G/9.98G [00:23<00:03, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  88%|█▊| 8.75G/9.98G [00:23<00:03, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  88%|█▊| 8.79G/9.98G [00:23<00:03, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  88%|█▊| 8.83G/9.98G [00:23<00:03, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  89%|█▊| 8.87G/9.98G [00:23<00:02, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  89%|█▊| 8.91G/9.98G [00:23<00:02, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|█▊| 8.95G/9.98G [00:24<00:02, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|█▊| 9.00G/9.98G [00:24<00:02, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█▊| 9.04G/9.98G [00:24<00:02, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█▊| 9.08G/9.98G [00:24<00:02, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█▊| 9.12G/9.98G [00:24<00:02, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█▊| 9.16G/9.98G [00:24<00:02, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█▊| 9.21G/9.98G [00:24<00:02, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█▊| 9.25G/9.98G [00:24<00:01, 382MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█▊| 9.29G/9.98G [00:24<00:01, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█▊| 9.33G/9.98G [00:25<00:01, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█▉| 9.37G/9.98G [00:25<00:01, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█▉| 9.42G/9.98G [00:25<00:01, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█▉| 9.46G/9.98G [00:25<00:01, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█▉| 9.50G/9.98G [00:25<00:01, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█▉| 9.54G/9.98G [00:25<00:01, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█▉| 9.58G/9.98G [00:25<00:01, 374MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█▉| 9.63G/9.98G [00:25<00:00, 373MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█▉| 9.67G/9.98G [00:25<00:00, 370MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█▉| 9.71G/9.98G [00:26<00:00, 368MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  98%|█▉| 9.75G/9.98G [00:26<00:00, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  98%|█▉| 9.79G/9.98G [00:26<00:00, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█▉| 9.84G/9.98G [00:26<00:00, 363MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█▉| 9.88G/9.98G [00:26<00:00, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█▉| 9.92G/9.98G [00:26<00:00, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██| 9.98G/9.98G [00:26<00:00, 372MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 1/2 [00:27<00:27, 27.03s/it]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|  | 21.0M/3.50G [00:00<00:21, 163MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   1%|  | 52.4M/3.50G [00:00<00:15, 226MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   3%|  | 94.4M/3.50G [00:00<00:11, 285MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   4%|   | 136M/3.50G [00:00<00:10, 316MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▏  | 178M/3.50G [00:00<00:09, 341MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▏  | 220M/3.50G [00:00<00:09, 356MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▏  | 262M/3.50G [00:00<00:08, 369MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▎  | 304M/3.50G [00:00<00:08, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  10%|▎  | 346M/3.50G [00:01<00:08, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  11%|▎  | 388M/3.50G [00:01<00:08, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  12%|▎  | 430M/3.50G [00:01<00:08, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  13%|▍  | 472M/3.50G [00:01<00:08, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  15%|▍  | 514M/3.50G [00:01<00:08, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  16%|▍  | 556M/3.50G [00:01<00:08, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  17%|▌  | 598M/3.50G [00:01<00:07, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  18%|▌  | 640M/3.50G [00:01<00:07, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  19%|▌  | 682M/3.50G [00:01<00:07, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  21%|▌  | 724M/3.50G [00:02<00:07, 380MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  22%|▋  | 765M/3.50G [00:02<00:07, 381MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  23%|▋  | 807M/3.50G [00:02<00:07, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  24%|▋  | 849M/3.50G [00:02<00:06, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  25%|▊  | 891M/3.50G [00:02<00:06, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  27%|▊  | 933M/3.50G [00:02<00:06, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  28%|▊  | 975M/3.50G [00:02<00:06, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  29%|▌ | 1.02G/3.50G [00:02<00:06, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  30%|▌ | 1.06G/3.50G [00:02<00:06, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  31%|▋ | 1.10G/3.50G [00:02<00:06, 395MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  33%|▋ | 1.14G/3.50G [00:03<00:06, 392MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  34%|▋ | 1.18G/3.50G [00:03<00:05, 393MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  35%|▋ | 1.23G/3.50G [00:03<00:05, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  36%|▋ | 1.27G/3.50G [00:03<00:05, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  37%|▋ | 1.31G/3.50G [00:03<00:05, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  39%|▊ | 1.35G/3.50G [00:03<00:05, 375MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  40%|▊ | 1.39G/3.50G [00:03<00:05, 379MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  41%|▊ | 1.44G/3.50G [00:03<00:05, 384MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  42%|▊ | 1.48G/3.50G [00:03<00:05, 388MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  43%|▊ | 1.52G/3.50G [00:04<00:05, 390MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  45%|▉ | 1.56G/3.50G [00:04<00:04, 391MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  46%|▉ | 1.60G/3.50G [00:04<00:07, 248MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00002.safetensors:  47%|▉ | 1.65G/3.50G [00:04<00:06, 280MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  48%|▉ | 1.69G/3.50G [00:04<00:05, 307MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  49%|▉ | 1.73G/3.50G [00:04<00:05, 325MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█ | 1.77G/3.50G [00:04<00:05, 345MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█ | 1.81G/3.50G [00:05<00:04, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█ | 1.86G/3.50G [00:05<00:04, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█ | 1.90G/3.50G [00:05<00:04, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█ | 1.94G/3.50G [00:05<00:04, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█▏| 1.98G/3.50G [00:05<00:03, 387MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█▏| 2.02G/3.50G [00:05<00:03, 383MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█▏| 2.07G/3.50G [00:05<00:03, 377MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█▏| 2.11G/3.50G [00:05<00:03, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  61%|█▏| 2.15G/3.50G [00:05<00:03, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  63%|█▎| 2.19G/3.50G [00:06<00:03, 368MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  64%|█▎| 2.23G/3.50G [00:06<00:03, 366MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  65%|█▎| 2.28G/3.50G [00:06<00:03, 363MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  66%|█▎| 2.32G/3.50G [00:06<00:03, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  67%|█▎| 2.36G/3.50G [00:06<00:03, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  69%|█▎| 2.40G/3.50G [00:06<00:03, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  70%|█▍| 2.44G/3.50G [00:06<00:02, 354MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  71%|█▍| 2.49G/3.50G [00:06<00:02, 351MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  72%|█▍| 2.53G/3.50G [00:06<00:02, 352MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  73%|█▍| 2.57G/3.50G [00:07<00:02, 353MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  75%|█▍| 2.61G/3.50G [00:07<00:02, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  76%|█▌| 2.65G/3.50G [00:07<00:02, 358MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  77%|█▌| 2.69G/3.50G [00:07<00:02, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  78%|█▌| 2.74G/3.50G [00:07<00:02, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  79%|█▌| 2.78G/3.50G [00:07<00:02, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  81%|█▌| 2.82G/3.50G [00:07<00:01, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  82%|█▋| 2.86G/3.50G [00:07<00:01, 359MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  83%|█▋| 2.90G/3.50G [00:08<00:01, 365MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  84%|█▋| 2.95G/3.50G [00:08<00:01, 372MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  85%|█▋| 2.99G/3.50G [00:08<00:01, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  87%|█▋| 3.03G/3.50G [00:08<00:01, 378MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  88%|█▊| 3.07G/3.50G [00:08<00:01, 376MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  89%|█▊| 3.11G/3.50G [00:08<00:01, 371MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  90%|█▊| 3.16G/3.50G [00:08<00:00, 367MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█▊| 3.20G/3.50G [00:08<00:00, 362MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█▊| 3.24G/3.50G [00:08<00:00, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█▉| 3.28G/3.50G [00:09<00:00, 357MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█▉| 3.32G/3.50G [00:09<00:00, 356MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█▉| 3.37G/3.50G [00:09<00:00, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█▉| 3.41G/3.50G [00:09<00:00, 361MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█▉| 3.45G/3.50G [00:09<00:00, 360MB/s]\u001b[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██| 3.50G/3.50G [00:09<00:00, 362MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:36<00:00, 18.38s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:36<00:00, 18.39s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:36<00:00, 18.38s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:36<00:00, 18.38s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.81s/it]\n",
      "Downloading (…)neration_config.json: 100%|█████| 188/188 [00:00<00:00, 1.40MB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.14s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.14s/it]\n",
      "Number of update steps per epoch 763\n",
      "  6%|██▌                                     | 48/763 [19:27<4:48:15, 24.19s/it]"
     ]
    }
   ],
   "source": [
    "! torchrun --nnodes 1 \\\n",
    "        --nproc_per_node 4 \\\n",
    "        --master_addr localhost \\\n",
    "        --master_port 7777 scripts/run_clm_no_trainer.py \\\n",
    "        --bf16 True \\\n",
    "        --dataset_path processed/data \\\n",
    "        --output_dir model \\\n",
    "        --epochs 3 \\\n",
    "        --fsdp \"full_shard auto_wrap\" \\\n",
    "        --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer \\\n",
    "        --gradient_checkpointing True \\\n",
    "        --model_id meta-llama/Llama-2-7b-chat-hf \\\n",
    "        --optimizer adamw_torch \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --access_token {access_token} \\\n",
    "        --max_steps 50 \\\n",
    "        --cache_dir /home/ec2-user/SageMaker/cache \\\n",
    "        --model_dir model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training finishes the model files will be save under the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the Llama V2 model using FSDP on Amazon SageMaker\n",
    "\n",
    "We will begin by uploading the tokenized data to S3 which will be uploaded to the training cluster during training.\n",
    "\n",
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/processed/data/'\n",
    "print(f\"training dataset to: {training_input_path}\")# save train_dataset to s3\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning, we will use Amazon SageMaker and PyTorch FSDP to train our model. Amazon SageMaker makes it easy to create a multi-node cluster to train our model in a distributed manner. The `sagemaker` python SDK supports to run training jobs using `torchrun`, to distribute the script across multiple nodes and GPUs. \n",
    "\n",
    "To use `torchrun` to execute our scripts, we only have to define the `distribution` parameter in our Estimator and set it to `\"torch_distributed\": {\"enabled\": True}`. This tells sagemaker to launch our training job with.\n",
    "\n",
    "```python\n",
    "torchrun --nnodes 1 --nproc_per_node 4 --master_addr algo-1 --master_port 7777  scripts/run_clm_no_trainer.py --bf16 True --dataset_path processed/data  --output_dir model --epochs 3 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --gradient_checkpointing True --model_id meta-llama/Llama-2-7b-chat-hf --optimizer adamw_torch --per_device_train_batch_size l```\n",
    "\n",
    "To use FSDP with the Hugging Face Trainer, we need to provide our `fsdp` strategy as well as the `transformer layer policy`. \n",
    "\n",
    "In our example, we will use `full shard auto_wrap` and `LlamaDecoderLayer` as transformer layer policy. If you run this example and change the model id make sure to also adjust the transformer layer policy. \n",
    "\n",
    "We prepared a run_clm.py, which implements causal language modeling and accepts our fsdp and other hyperparameters.\n",
    "\n",
    "To create a sagemaker training job, we create an `HuggingFace` Estimator and provide all our information. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': model_id, # model id from huggingface.co/models\n",
    "    'dataset_path': '/opt/ml/input/data/train', # path where sagemaker will save training dataset\n",
    "    'valid_path':\"/opt/ml/input/data/valid\",\n",
    "    'gradient_checkpointing': True, # enable gradient checkpointing\n",
    "    'bf16': True, # enable mixed precision training\n",
    "    'optimizer': \"adamw_torch\", # optimizer\n",
    "    'per_device_train_batch_size': 1, # batch size per device during training\n",
    "    'epochs': 1, # number of epochs to train\n",
    "    'fsdp': '\"full_shard auto_wrap\"', # fully sharded data parallelism\n",
    "    'fsdp_transformer_layer_cls_to_wrap': \"LlamaDecoderLayer\", # transformer layer to wrap\n",
    "    'max_steps':50,\n",
    "    'access_token': access_token\n",
    "}\n",
    "\n",
    "# This environment variables are useful when training with P4d inorder to enable EFA based training.\n",
    "env = {}\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "\n",
    "# estimator \n",
    "huggingface_estimator = PyTorch(\n",
    "    entry_point='run_clm_no_trainer.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    #image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.0.1-gpu-py310-cu118-ubuntu20.04-sagemaker\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    framework_version='2.0.1',\n",
    "    py_version=\"py310\",\n",
    "    #environment=env,\n",
    "    hyperparameters = hyperparameters,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-09-06-18-58-08-019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-06 18:58:08 Starting - Starting the training job...\n",
      "2023-09-06 18:58:23 Downloading - Downloading input data\n",
      "2023-09-06 18:58:23 Training - Training image download completed. Training in progress.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-09-06 18:58:25,241 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-09-06 18:58:25,273 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-06 18:58:25,281 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-09-06 18:58:25,284 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-09-06 18:58:25,285 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-09-06 18:58:29,014 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.31 (from -r requirements.txt (line 1))\n",
      "Obtaining dependency information for transformers==4.31 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting datasets (from -r requirements.txt (line 2))\n",
      "Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate>=0.21 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\n",
      "Collecting bitsandbytes (from -r requirements.txt (line 4))\n",
      "Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/1e/2c/af22cd797fc368a9f098ed03015730e6568b884fe67f9940793d944a4b7b/bitsandbytes-0.41.1-py3-none-any.whl.metadata\n",
      "Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting peft (from -r requirements.txt (line 5))\n",
      "Obtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\n",
      "Downloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31->-r requirements.txt (line 1)) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31->-r requirements.txt (line 1)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.31->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31->-r requirements.txt (line 1))\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 71.4 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.31->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2.0.3)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 2))\n",
      "Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 2)) (2023.6.0)\n",
      "Collecting aiohttp (from datasets->-r requirements.txt (line 2))\n",
      "Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21->-r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 2))\n",
      "Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 27.7 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 2))\n",
      "Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 2))\n",
      "Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 42.7 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 2))\n",
      "Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 2))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31->-r requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31->-r requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.21->-r requirements.txt (line 3)) (1.3.0)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 99.0 MB/s eta 0:00:00\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 68.7 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 16.7 MB/s eta 0:00:00\n",
      "Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 21.6 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 76.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 50.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 63.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 86.5 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 42.5 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 43.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, safetensors, bitsandbytes, xxhash, regex, multidict, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, transformers, aiohttp, peft, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.41.1 datasets-2.14.5 frozenlist-1.4.0 huggingface-hub-0.16.4 multidict-6.0.4 peft-0.5.0 regex-2023.8.8 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.3.0 yarl-1.9.2\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2023-09-06 18:58:43,216 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-09-06 18:58:43,216 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-09-06 18:58:43,272 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-06 18:58:43,313 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-06 18:58:43,322 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-09-06 18:58:43,354 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-06 18:58:43,364 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"access_token\": \"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\",\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/train\",\n",
      "        \"epochs\": 1,\n",
      "        \"fsdp\": \"\\\"full_shard auto_wrap\\\"\",\n",
      "        \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"max_steps\": 100,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "        \"optimizer\": \"adamw_torch\",\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"valid_path\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-09-06-18-58-08-019\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-365792799466/pytorch-training-2023-09-06-18-58-08-019/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm_no_trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm_no_trainer.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"access_token\":\"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\",\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/train\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"fsdp_transformer_layer_cls_to_wrap\":\"LlamaDecoderLayer\",\"gradient_checkpointing\":true,\"max_steps\":100,\"model_id\":\"meta-llama/Llama-2-7b-chat-hf\",\"optimizer\":\"adamw_torch\",\"per_device_train_batch_size\":1,\"valid_path\":\"/opt/ml/input/data/valid\"}\n",
      "SM_USER_ENTRY_POINT=run_clm_no_trainer.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm_no_trainer\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=48\n",
      "SM_NUM_GPUS=4\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-west-2-365792799466/pytorch-training-2023-09-06-18-58-08-019/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"access_token\":\"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\",\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/train\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"fsdp_transformer_layer_cls_to_wrap\":\"LlamaDecoderLayer\",\"gradient_checkpointing\":true,\"max_steps\":100,\"model_id\":\"meta-llama/Llama-2-7b-chat-hf\",\"optimizer\":\"adamw_torch\",\"per_device_train_batch_size\":1,\"valid_path\":\"/opt/ml/input/data/valid\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-09-06-18-58-08-019\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-365792799466/pytorch-training-2023-09-06-18-58-08-019/source/sourcedir.tar.gz\",\"module_name\":\"run_clm_no_trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm_no_trainer.py\"}\n",
      "SM_USER_ARGS=[\"--access_token\",\"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\",\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/train\",\"--epochs\",\"1\",\"--fsdp\",\"\\\"full_shard auto_wrap\\\"\",\"--fsdp_transformer_layer_cls_to_wrap\",\"LlamaDecoderLayer\",\"--gradient_checkpointing\",\"True\",\"--max_steps\",\"100\",\"--model_id\",\"meta-llama/Llama-2-7b-chat-hf\",\"--optimizer\",\"adamw_torch\",\"--per_device_train_batch_size\",\"1\",\"--valid_path\",\"/opt/ml/input/data/valid\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_ACCESS_TOKEN=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\n",
      "SM_HP_BF16=true\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/train\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_FSDP=\"full_shard auto_wrap\"\n",
      "SM_HP_FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP=LlamaDecoderLayer\n",
      "SM_HP_GRADIENT_CHECKPOINTING=true\n",
      "SM_HP_MAX_STEPS=100\n",
      "SM_HP_MODEL_ID=meta-llama/Llama-2-7b-chat-hf\n",
      "SM_HP_OPTIMIZER=adamw_torch\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "SM_HP_VALID_PATH=/opt/ml/input/data/valid\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 4 run_clm_no_trainer.py --access_token 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 --bf16 True --dataset_path /opt/ml/input/data/train --epochs 1 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap LlamaDecoderLayer --gradient_checkpointing True --max_steps 100 --model_id meta-llama/Llama-2-7b-chat-hf --optimizer adamw_torch --per_device_train_batch_size 1 --valid_path /opt/ml/input/data/valid\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.83s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.83s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.02s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]\n",
      "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 9.33MB/s]\n",
      "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 21.9MB/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.37s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 45.3MB/s]\n",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 5.75MB/s]\n",
      "Number of update steps per epoch 763\n",
      "0%|          | 0/763 [00:00<?, ?it/s]\n",
      "NCCL version 2.17.1+cuda11.8\n",
      "algo-1:71:225 [2] nccl_net_ofi_init:1472 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-1:70:224 [1] nccl_net_ofi_init:1472 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-1:72:223 [3] nccl_net_ofi_init:1472 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-1:69:222 [0] nccl_net_ofi_init:1472 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "0%|          | 1/763 [00:38<8:04:11, 38.13s/it]\n",
      "0%|          | 2/763 [01:00<6:08:50, 29.08s/it]\n",
      "0%|          | 3/763 [01:24<5:34:35, 26.41s/it]\n",
      "1%|          | 4/763 [01:47<5:19:10, 25.23s/it]\n",
      "1%|          | 5/763 [02:10<5:08:48, 24.44s/it]\n",
      "1%|          | 6/763 [02:33<5:02:54, 24.01s/it]\n",
      "1%|          | 7/763 [02:57<5:00:42, 23.87s/it]\n",
      "1%|          | 8/763 [03:21<4:59:43, 23.82s/it]\n",
      "1%|          | 9/763 [03:44<4:58:42, 23.77s/it]\n",
      "1%|▏         | 10/763 [04:07<4:55:58, 23.58s/it]\n",
      "1%|▏         | 11/763 [04:31<4:54:15, 23.48s/it]\n",
      "2%|▏         | 12/763 [04:55<4:57:01, 23.73s/it]\n",
      "2%|▏         | 13/763 [05:19<4:57:33, 23.80s/it]\n",
      "2%|▏         | 14/763 [05:43<4:59:50, 24.02s/it]\n",
      "2%|▏         | 15/763 [06:07<4:57:29, 23.86s/it]\n",
      "2%|▏         | 16/763 [06:31<4:57:44, 23.91s/it]\n",
      "2%|▏         | 17/763 [06:55<4:56:36, 23.86s/it]\n",
      "2%|▏         | 18/763 [07:18<4:54:16, 23.70s/it]\n",
      "2%|▏         | 19/763 [07:40<4:49:13, 23.33s/it]\n",
      "3%|▎         | 20/763 [08:04<4:49:31, 23.38s/it]\n",
      "3%|▎         | 21/763 [08:28<4:53:13, 23.71s/it]\n",
      "3%|▎         | 22/763 [08:52<4:51:57, 23.64s/it]\n",
      "3%|▎         | 23/763 [09:15<4:49:22, 23.46s/it]\n",
      "3%|▎         | 24/763 [09:39<4:50:25, 23.58s/it]\n",
      "3%|▎         | 25/763 [10:02<4:48:32, 23.46s/it]\n",
      "3%|▎         | 26/763 [10:26<4:50:13, 23.63s/it]\n",
      "4%|▎         | 27/763 [10:50<4:50:07, 23.65s/it]\n",
      "4%|▎         | 28/763 [11:13<4:49:29, 23.63s/it]\n",
      "4%|▍         | 29/763 [11:38<4:52:16, 23.89s/it]\n",
      "4%|▍         | 30/763 [12:02<4:52:48, 23.97s/it]\n",
      "4%|▍         | 31/763 [12:26<4:52:10, 23.95s/it]\n",
      "4%|▍         | 32/763 [12:49<4:49:49, 23.79s/it]\n",
      "4%|▍         | 33/763 [13:11<4:43:40, 23.32s/it]\n",
      "4%|▍         | 34/763 [13:35<4:42:30, 23.25s/it]\n",
      "5%|▍         | 35/763 [13:58<4:43:33, 23.37s/it]\n",
      "5%|▍         | 36/763 [14:21<4:41:47, 23.26s/it]\n",
      "5%|▍         | 37/763 [14:44<4:38:14, 23.00s/it]\n",
      "5%|▍         | 38/763 [15:07<4:38:39, 23.06s/it]\n",
      "5%|▌         | 39/763 [15:31<4:41:32, 23.33s/it]\n",
      "5%|▌         | 40/763 [15:55<4:43:05, 23.49s/it]\n",
      "5%|▌         | 41/763 [16:19<4:46:05, 23.78s/it]\n",
      "6%|▌         | 42/763 [16:44<4:48:37, 24.02s/it]\n",
      "6%|▌         | 43/763 [17:08<4:48:44, 24.06s/it]\n",
      "6%|▌         | 44/763 [17:31<4:45:59, 23.87s/it]\n",
      "6%|▌         | 45/763 [17:56<4:48:55, 24.14s/it]\n",
      "6%|▌         | 46/763 [18:19<4:45:06, 23.86s/it]\n",
      "6%|▌         | 47/763 [18:43<4:45:27, 23.92s/it]\n",
      "6%|▋         | 48/763 [19:07<4:43:54, 23.83s/it]\n",
      "6%|▋         | 49/763 [19:30<4:42:41, 23.76s/it]\n",
      "7%|▋         | 50/763 [19:54<4:43:05, 23.82s/it]\n",
      "7%|▋         | 51/763 [20:18<4:42:36, 23.82s/it]\n",
      "7%|▋         | 52/763 [20:43<4:44:48, 24.03s/it]\n",
      "7%|▋         | 53/763 [21:07<4:45:43, 24.15s/it]\n",
      "7%|▋         | 54/763 [21:31<4:42:18, 23.89s/it]\n",
      "7%|▋         | 55/763 [21:55<4:42:54, 23.98s/it]\n",
      "7%|▋         | 56/763 [22:19<4:43:30, 24.06s/it]\n",
      "7%|▋         | 57/763 [22:43<4:41:19, 23.91s/it]\n",
      "8%|▊         | 58/763 [23:07<4:41:17, 23.94s/it]\n",
      "8%|▊         | 59/763 [23:30<4:40:43, 23.92s/it]\n",
      "8%|▊         | 60/763 [23:54<4:39:05, 23.82s/it]\n",
      "8%|▊         | 61/763 [24:18<4:38:00, 23.76s/it]\n",
      "8%|▊         | 62/763 [24:42<4:40:07, 23.98s/it]\n",
      "8%|▊         | 63/763 [25:06<4:38:38, 23.88s/it]\n",
      "8%|▊         | 64/763 [25:30<4:39:19, 23.98s/it]\n",
      "9%|▊         | 65/763 [25:54<4:38:07, 23.91s/it]\n",
      "9%|▊         | 66/763 [26:17<4:36:14, 23.78s/it]\n",
      "9%|▉         | 67/763 [26:40<4:34:03, 23.63s/it]\n",
      "9%|▉         | 68/763 [27:05<4:35:24, 23.78s/it]\n",
      "9%|▉         | 69/763 [27:28<4:34:11, 23.70s/it]\n",
      "9%|▉         | 70/763 [27:52<4:32:55, 23.63s/it]\n",
      "9%|▉         | 71/763 [28:16<4:34:51, 23.83s/it]\n",
      "9%|▉         | 72/763 [28:39<4:31:23, 23.56s/it]\n",
      "10%|▉         | 73/763 [29:02<4:28:54, 23.38s/it]\n",
      "10%|▉         | 74/763 [29:25<4:27:18, 23.28s/it]\n",
      "10%|▉         | 75/763 [29:48<4:28:12, 23.39s/it]\n",
      "10%|▉         | 76/763 [30:12<4:29:44, 23.56s/it]\n",
      "10%|█         | 77/763 [30:36<4:31:02, 23.71s/it]\n",
      "10%|█         | 78/763 [31:01<4:31:53, 23.82s/it]\n",
      "10%|█         | 79/763 [31:24<4:31:21, 23.80s/it]\n",
      "10%|█         | 80/763 [31:47<4:28:26, 23.58s/it]\n",
      "11%|█         | 81/763 [32:11<4:28:46, 23.65s/it]\n",
      "11%|█         | 82/763 [32:35<4:29:35, 23.75s/it]\n",
      "11%|█         | 83/763 [32:59<4:29:25, 23.77s/it]\n",
      "11%|█         | 84/763 [33:23<4:28:15, 23.71s/it]\n",
      "11%|█         | 85/763 [33:46<4:25:34, 23.50s/it]\n",
      "11%|█▏        | 86/763 [34:10<4:27:08, 23.68s/it]\n",
      "11%|█▏        | 87/763 [34:34<4:27:44, 23.76s/it]\n",
      "12%|█▏        | 88/763 [34:58<4:27:47, 23.80s/it]\n",
      "12%|█▏        | 89/763 [35:20<4:24:16, 23.53s/it]\n",
      "12%|█▏        | 90/763 [35:44<4:23:40, 23.51s/it]\n",
      "12%|█▏        | 91/763 [36:08<4:24:18, 23.60s/it]\n",
      "12%|█▏        | 92/763 [36:32<4:24:51, 23.68s/it]\n",
      "12%|█▏        | 93/763 [36:55<4:23:08, 23.56s/it]\n",
      "12%|█▏        | 94/763 [37:19<4:23:53, 23.67s/it]\n",
      "12%|█▏        | 95/763 [37:42<4:23:15, 23.65s/it]\n",
      "13%|█▎        | 96/763 [38:06<4:23:25, 23.70s/it]\n",
      "13%|█▎        | 97/763 [38:30<4:24:46, 23.85s/it]\n",
      "13%|█▎        | 98/763 [38:55<4:25:25, 23.95s/it]\n",
      "13%|█▎        | 99/763 [39:18<4:24:25, 23.89s/it]\n",
      "13%|█▎        | 100/763 [39:42<4:24:09, 23.91s/it]\n",
      "13%|█▎        | 100/763 [40:06<4:25:53, 24.06s/it]\n",
      "******epoch=0: train_ppl=tensor(2.6901, device='cuda:0') train_loss=tensor(0.9896, device='cuda:0')******\n",
      "0%|          | 0/38 [00:00<?, ?it/s]\n",
      "3%|▎         | 1/38 [00:03<02:02,  3.31s/it]\n",
      "5%|▌         | 2/38 [00:06<01:50,  3.08s/it]\n",
      "8%|▊         | 3/38 [00:09<01:45,  3.00s/it]\n",
      "11%|█         | 4/38 [00:12<01:41,  2.97s/it]\n",
      "13%|█▎        | 5/38 [00:14<01:37,  2.95s/it]\n",
      "16%|█▌        | 6/38 [00:17<01:34,  2.94s/it]\n",
      "18%|█▊        | 7/38 [00:20<01:30,  2.93s/it]\n",
      "21%|██        | 8/38 [00:23<01:27,  2.93s/it]\n",
      "24%|██▎       | 9/38 [00:26<01:25,  2.93s/it]\n",
      "26%|██▋       | 10/38 [00:29<01:22,  2.93s/it]\n",
      "29%|██▉       | 11/38 [00:32<01:18,  2.93s/it]\n",
      "32%|███▏      | 12/38 [00:35<01:15,  2.92s/it]\n",
      "34%|███▍      | 13/38 [00:38<01:12,  2.91s/it]\n",
      "37%|███▋      | 14/38 [00:41<01:09,  2.92s/it]\n",
      "39%|███▉      | 15/38 [00:44<01:07,  2.92s/it]\n",
      "42%|████▏     | 16/38 [00:47<01:04,  2.92s/it]\n",
      "45%|████▍     | 17/38 [00:49<01:01,  2.91s/it]\n",
      "47%|████▋     | 18/38 [00:52<00:58,  2.92s/it]\n",
      "50%|█████     | 19/38 [00:55<00:55,  2.92s/it]\n",
      "53%|█████▎    | 20/38 [00:58<00:52,  2.92s/it]\n",
      "55%|█████▌    | 21/38 [01:01<00:49,  2.92s/it]\n",
      "58%|█████▊    | 22/38 [01:04<00:46,  2.92s/it]\n",
      "61%|██████    | 23/38 [01:07<00:43,  2.92s/it]\n",
      "63%|██████▎   | 24/38 [01:10<00:40,  2.92s/it]\n",
      "66%|██████▌   | 25/38 [01:13<00:37,  2.92s/it]\n",
      "68%|██████▊   | 26/38 [01:16<00:35,  2.92s/it]\n",
      "71%|███████   | 27/38 [01:19<00:32,  2.92s/it]\n",
      "74%|███████▎  | 28/38 [01:22<00:29,  2.92s/it]\n",
      "76%|███████▋  | 29/38 [01:25<00:26,  2.92s/it]\n",
      "79%|███████▉  | 30/38 [01:27<00:23,  2.92s/it]\n",
      "82%|████████▏ | 31/38 [01:30<00:20,  2.92s/it]\n",
      "84%|████████▍ | 32/38 [01:33<00:17,  2.92s/it]\n",
      "87%|████████▋ | 33/38 [01:36<00:14,  2.92s/it]\n",
      "89%|████████▉ | 34/38 [01:39<00:11,  2.92s/it]\n",
      "92%|█████████▏| 35/38 [01:42<00:08,  2.92s/it]\n",
      "95%|█████████▍| 36/38 [01:45<00:05,  2.92s/it]\n",
      "97%|█████████▋| 37/38 [01:48<00:02,  2.92s/it]\n",
      "100%|██████████| 38/38 [01:51<00:00,  2.92s/it]\n",
      "100%|██████████| 38/38 [01:51<00:00,  2.93s/it]\n",
      "*******epoch=0: eval_ppl=tensor(2.4658, device='cuda:0') eval_loss=tensor(0.9025, device='cuda:0')*******\n",
      "0%|          | 0/763 [00:00<?, ?it/s]\n",
      "0%|          | 1/763 [00:23<5:01:58, 23.78s/it]\n",
      "0%|          | 2/763 [00:47<5:01:35, 23.78s/it]\n",
      "0%|          | 3/763 [01:11<5:02:28, 23.88s/it]\n",
      "1%|          | 4/763 [01:34<4:55:29, 23.36s/it]\n",
      "1%|          | 5/763 [01:58<4:58:04, 23.59s/it]\n",
      "1%|          | 6/763 [02:21<4:56:11, 23.48s/it]\n",
      "1%|          | 7/763 [02:43<4:51:55, 23.17s/it]\n",
      "1%|          | 8/763 [03:06<4:50:08, 23.06s/it]\n",
      "1%|          | 9/763 [03:29<4:48:37, 22.97s/it]\n",
      "1%|▏         | 10/763 [03:52<4:46:34, 22.83s/it]\n",
      "1%|▏         | 11/763 [04:15<4:46:46, 22.88s/it]\n",
      "2%|▏         | 12/763 [04:37<4:46:20, 22.88s/it]\n",
      "2%|▏         | 13/763 [05:01<4:47:49, 23.03s/it]\n",
      "2%|▏         | 14/763 [05:24<4:48:16, 23.09s/it]\n",
      "2%|▏         | 15/763 [05:48<4:49:52, 23.25s/it]\n",
      "2%|▏         | 16/763 [06:10<4:46:57, 23.05s/it]\n",
      "2%|▏         | 17/763 [06:33<4:45:03, 22.93s/it]\n",
      "2%|▏         | 18/763 [06:57<4:48:25, 23.23s/it]\n",
      "2%|▏         | 19/763 [07:20<4:48:39, 23.28s/it]\n",
      "3%|▎         | 20/763 [07:43<4:46:32, 23.14s/it]\n",
      "3%|▎         | 21/763 [08:06<4:45:38, 23.10s/it]\n",
      "3%|▎         | 22/763 [08:29<4:44:53, 23.07s/it]\n",
      "3%|▎         | 23/763 [08:51<4:42:10, 22.88s/it]\n",
      "3%|▎         | 24/763 [09:15<4:42:53, 22.97s/it]\n",
      "3%|▎         | 25/763 [09:39<4:47:42, 23.39s/it]\n",
      "3%|▎         | 26/763 [10:03<4:49:22, 23.56s/it]\n",
      "4%|▎         | 27/763 [10:26<4:47:37, 23.45s/it]\n",
      "4%|▎         | 28/763 [10:50<4:49:50, 23.66s/it]\n",
      "4%|▍         | 29/763 [11:14<4:50:03, 23.71s/it]\n",
      "4%|▍         | 30/763 [11:38<4:49:39, 23.71s/it]\n",
      "4%|▍         | 31/763 [12:01<4:47:58, 23.60s/it]\n",
      "4%|▍         | 32/763 [12:24<4:43:18, 23.25s/it]\n",
      "4%|▍         | 33/763 [12:46<4:40:58, 23.09s/it]\n",
      "4%|▍         | 34/763 [13:10<4:41:49, 23.20s/it]\n",
      "5%|▍         | 35/763 [13:33<4:40:38, 23.13s/it]\n",
      "5%|▍         | 36/763 [13:56<4:40:19, 23.14s/it]\n",
      "5%|▍         | 37/763 [14:19<4:40:44, 23.20s/it]\n",
      "5%|▍         | 38/763 [14:42<4:40:13, 23.19s/it]\n",
      "5%|▌         | 39/763 [15:05<4:39:11, 23.14s/it]\n",
      "5%|▌         | 40/763 [15:28<4:36:24, 22.94s/it]\n",
      "5%|▌         | 41/763 [15:51<4:37:27, 23.06s/it]\n",
      "6%|▌         | 42/763 [16:15<4:37:55, 23.13s/it]\n",
      "6%|▌         | 43/763 [16:37<4:35:13, 22.94s/it]\n",
      "6%|▌         | 44/763 [17:00<4:35:50, 23.02s/it]\n",
      "6%|▌         | 45/763 [17:23<4:35:44, 23.04s/it]\n",
      "6%|▌         | 46/763 [17:47<4:36:35, 23.15s/it]\n",
      "6%|▌         | 47/763 [18:10<4:35:59, 23.13s/it]\n",
      "6%|▋         | 48/763 [18:33<4:35:20, 23.11s/it]\n",
      "6%|▋         | 49/763 [18:56<4:35:52, 23.18s/it]\n",
      "7%|▋         | 50/763 [19:19<4:35:01, 23.14s/it]\n",
      "7%|▋         | 51/763 [19:43<4:35:49, 23.24s/it]\n",
      "7%|▋         | 52/763 [20:06<4:37:07, 23.39s/it]\n",
      "7%|▋         | 53/763 [20:30<4:37:55, 23.49s/it]\n",
      "7%|▋         | 54/763 [20:53<4:36:15, 23.38s/it]\n",
      "7%|▋         | 55/763 [21:16<4:33:07, 23.15s/it]\n",
      "7%|▋         | 56/763 [21:39<4:30:46, 22.98s/it]\n",
      "7%|▋         | 57/763 [22:02<4:30:48, 23.01s/it]\n",
      "8%|▊         | 58/763 [22:25<4:30:39, 23.03s/it]\n",
      "8%|▊         | 59/763 [22:48<4:32:08, 23.19s/it]\n",
      "8%|▊         | 60/763 [23:12<4:33:05, 23.31s/it]\n",
      "8%|▊         | 61/763 [23:35<4:31:45, 23.23s/it]\n",
      "8%|▊         | 62/763 [23:58<4:31:47, 23.26s/it]\n",
      "8%|▊         | 63/763 [24:21<4:30:04, 23.15s/it]\n",
      "8%|▊         | 64/763 [24:44<4:27:46, 22.99s/it]\n",
      "9%|▊         | 65/763 [25:07<4:29:27, 23.16s/it]\n",
      "9%|▊         | 66/763 [25:30<4:28:01, 23.07s/it]\n",
      "9%|▉         | 67/763 [25:53<4:27:24, 23.05s/it]\n",
      "9%|▉         | 68/763 [26:16<4:25:34, 22.93s/it]\n",
      "9%|▉         | 69/763 [26:39<4:26:08, 23.01s/it]\n",
      "9%|▉         | 70/763 [27:02<4:26:13, 23.05s/it]\n",
      "9%|▉         | 71/763 [27:26<4:28:32, 23.28s/it]\n",
      "9%|▉         | 72/763 [27:50<4:30:22, 23.48s/it]\n",
      "10%|▉         | 73/763 [28:13<4:29:34, 23.44s/it]\n",
      "10%|▉         | 74/763 [28:36<4:27:26, 23.29s/it]\n",
      "10%|▉         | 75/763 [28:59<4:25:48, 23.18s/it]\n",
      "10%|▉         | 76/763 [29:21<4:22:21, 22.91s/it]\n",
      "10%|█         | 77/763 [29:44<4:21:33, 22.88s/it]\n",
      "10%|█         | 78/763 [30:09<4:27:21, 23.42s/it]\n",
      "10%|█         | 79/763 [30:32<4:26:49, 23.41s/it]\n",
      "10%|█         | 80/763 [30:55<4:22:56, 23.10s/it]\n",
      "11%|█         | 81/763 [31:17<4:21:07, 22.97s/it]\n",
      "11%|█         | 82/763 [31:40<4:21:27, 23.04s/it]\n",
      "11%|█         | 83/763 [32:03<4:20:46, 23.01s/it]\n",
      "11%|█         | 84/763 [32:26<4:18:44, 22.86s/it]\n",
      "11%|█         | 85/763 [32:49<4:18:41, 22.89s/it]\n",
      "11%|█▏        | 86/763 [33:12<4:19:43, 23.02s/it]\n",
      "11%|█▏        | 87/763 [33:36<4:21:07, 23.18s/it]\n",
      "12%|█▏        | 88/763 [34:00<4:22:51, 23.36s/it]\n",
      "12%|█▏        | 89/763 [34:23<4:23:18, 23.44s/it]\n",
      "12%|█▏        | 90/763 [34:48<4:26:34, 23.77s/it]\n",
      "12%|█▏        | 91/763 [35:11<4:26:06, 23.76s/it]\n",
      "12%|█▏        | 92/763 [35:35<4:24:11, 23.62s/it]\n",
      "12%|█▏        | 93/763 [35:58<4:22:48, 23.54s/it]\n",
      "12%|█▏        | 94/763 [36:21<4:21:56, 23.49s/it]\n",
      "12%|█▏        | 95/763 [36:44<4:18:10, 23.19s/it]\n",
      "13%|█▎        | 96/763 [37:07<4:17:42, 23.18s/it]\n",
      "13%|█▎        | 97/763 [37:30<4:16:04, 23.07s/it]\n",
      "13%|█▎        | 98/763 [37:53<4:15:05, 23.02s/it]\n",
      "13%|█▎        | 99/763 [38:15<4:12:41, 22.83s/it]\n",
      "13%|█▎        | 100/763 [38:38<4:13:33, 22.95s/it]\n",
      "13%|█▎        | 100/763 [39:01<4:18:47, 23.42s/it]\n",
      "******epoch=1: train_ppl=tensor(1.5605, device='cuda:0') train_loss=tensor(0.4450, device='cuda:0')******\n",
      "0%|          | 0/38 [00:00<?, ?it/s]\n",
      "3%|▎         | 1/38 [00:03<02:03,  3.34s/it]\n",
      "5%|▌         | 2/38 [00:06<01:51,  3.11s/it]\n",
      "8%|▊         | 3/38 [00:09<01:45,  3.02s/it]\n",
      "11%|█         | 4/38 [00:12<01:41,  2.98s/it]\n",
      "13%|█▎        | 5/38 [00:15<01:37,  2.96s/it]\n",
      "16%|█▌        | 6/38 [00:17<01:34,  2.95s/it]\n",
      "18%|█▊        | 7/38 [00:20<01:31,  2.94s/it]\n",
      "21%|██        | 8/38 [00:23<01:27,  2.93s/it]\n",
      "24%|██▎       | 9/38 [00:26<01:24,  2.93s/it]\n",
      "26%|██▋       | 10/38 [00:29<01:21,  2.92s/it]\n",
      "29%|██▉       | 11/38 [00:32<01:18,  2.92s/it]\n",
      "32%|███▏      | 12/38 [00:35<01:15,  2.91s/it]\n",
      "34%|███▍      | 13/38 [00:38<01:12,  2.92s/it]\n",
      "37%|███▋      | 14/38 [00:41<01:10,  2.92s/it]\n",
      "39%|███▉      | 15/38 [00:44<01:07,  2.92s/it]\n",
      "42%|████▏     | 16/38 [00:47<01:04,  2.92s/it]\n",
      "45%|████▍     | 17/38 [00:50<01:01,  2.92s/it]\n",
      "47%|████▋     | 18/38 [00:52<00:58,  2.92s/it]\n",
      "50%|█████     | 19/38 [00:55<00:55,  2.92s/it]\n",
      "53%|█████▎    | 20/38 [00:58<00:52,  2.91s/it]\n",
      "55%|█████▌    | 21/38 [01:01<00:49,  2.92s/it]\n",
      "58%|█████▊    | 22/38 [01:04<00:46,  2.92s/it]\n",
      "61%|██████    | 23/38 [01:07<00:43,  2.92s/it]\n",
      "63%|██████▎   | 24/38 [01:10<00:40,  2.92s/it]\n",
      "66%|██████▌   | 25/38 [01:13<00:37,  2.92s/it]\n",
      "68%|██████▊   | 26/38 [01:16<00:35,  2.92s/it]\n",
      "71%|███████   | 27/38 [01:19<00:32,  2.92s/it]\n",
      "74%|███████▎  | 28/38 [01:22<00:29,  2.92s/it]\n",
      "76%|███████▋  | 29/38 [01:25<00:26,  2.93s/it]\n",
      "79%|███████▉  | 30/38 [01:28<00:23,  2.93s/it]\n",
      "82%|████████▏ | 31/38 [01:30<00:20,  2.92s/it]\n",
      "84%|████████▍ | 32/38 [01:33<00:17,  2.92s/it]\n",
      "87%|████████▋ | 33/38 [01:36<00:14,  2.92s/it]\n",
      "89%|████████▉ | 34/38 [01:39<00:11,  2.92s/it]\n",
      "92%|█████████▏| 35/38 [01:42<00:08,  2.92s/it]\n",
      "95%|█████████▍| 36/38 [01:45<00:05,  2.92s/it]\n",
      "97%|█████████▋| 37/38 [01:48<00:02,  2.92s/it]\n",
      "100%|██████████| 38/38 [01:51<00:00,  2.92s/it]\n",
      "100%|██████████| 38/38 [01:51<00:00,  2.93s/it]\n",
      "*******epoch=1: eval_ppl=tensor(2.9519, device='cuda:0') eval_loss=tensor(1.0825, device='cuda:0')*******\n",
      "0%|          | 0/763 [00:00<?, ?it/s]\n",
      "0%|          | 1/763 [00:22<4:49:12, 22.77s/it]\n",
      "0%|          | 2/763 [00:46<4:53:39, 23.15s/it]\n",
      "0%|          | 3/763 [01:10<4:57:34, 23.49s/it]\n",
      "1%|          | 4/763 [01:33<4:56:48, 23.46s/it]\n",
      "1%|          | 5/763 [01:57<4:57:06, 23.52s/it]\n",
      "1%|          | 6/763 [02:20<4:54:55, 23.38s/it]\n",
      "1%|          | 7/763 [02:42<4:51:23, 23.13s/it]\n",
      "1%|          | 8/763 [03:06<4:51:11, 23.14s/it]\n",
      "1%|          | 9/763 [03:28<4:49:35, 23.04s/it]\n",
      "1%|▏         | 10/763 [03:51<4:49:05, 23.04s/it]\n",
      "1%|▏         | 11/763 [04:14<4:46:03, 22.82s/it]\n",
      "2%|▏         | 12/763 [04:37<4:47:58, 23.01s/it]\n",
      "2%|▏         | 13/763 [05:00<4:46:10, 22.89s/it]\n",
      "2%|▏         | 14/763 [05:23<4:47:50, 23.06s/it]\n",
      "2%|▏         | 15/763 [05:46<4:46:04, 22.95s/it]\n",
      "2%|▏         | 16/763 [06:09<4:46:54, 23.04s/it]\n",
      "2%|▏         | 17/763 [06:33<4:50:17, 23.35s/it]\n",
      "2%|▏         | 18/763 [06:56<4:48:16, 23.22s/it]\n",
      "2%|▏         | 19/763 [07:19<4:44:52, 22.97s/it]\n",
      "3%|▎         | 20/763 [07:42<4:45:25, 23.05s/it]\n",
      "3%|▎         | 21/763 [08:06<4:48:20, 23.32s/it]\n",
      "3%|▎         | 22/763 [08:29<4:46:44, 23.22s/it]\n",
      "3%|▎         | 23/763 [08:52<4:45:34, 23.15s/it]\n",
      "3%|▎         | 24/763 [09:14<4:43:18, 23.00s/it]\n",
      "3%|▎         | 25/763 [09:37<4:40:49, 22.83s/it]\n",
      "3%|▎         | 26/763 [09:59<4:39:58, 22.79s/it]\n",
      "4%|▎         | 27/763 [10:22<4:38:21, 22.69s/it]\n",
      "4%|▎         | 28/763 [10:45<4:39:30, 22.82s/it]\n",
      "4%|▍         | 29/763 [11:08<4:39:06, 22.82s/it]\n",
      "4%|▍         | 30/763 [11:32<4:42:55, 23.16s/it]\n",
      "4%|▍         | 31/763 [11:57<4:48:43, 23.67s/it]\n",
      "4%|▍         | 32/763 [12:22<4:53:12, 24.07s/it]\n",
      "4%|▍         | 33/763 [12:47<4:56:05, 24.34s/it]\n",
      "4%|▍         | 34/763 [13:11<4:54:24, 24.23s/it]\n",
      "5%|▍         | 35/763 [13:33<4:48:04, 23.74s/it]\n",
      "5%|▍         | 36/763 [13:57<4:46:14, 23.62s/it]\n",
      "5%|▍         | 37/763 [14:20<4:44:24, 23.51s/it]\n",
      "5%|▍         | 38/763 [14:43<4:42:51, 23.41s/it]\n",
      "5%|▌         | 39/763 [15:06<4:40:57, 23.28s/it]\n",
      "5%|▌         | 40/763 [15:30<4:43:08, 23.50s/it]\n",
      "5%|▌         | 41/763 [15:52<4:39:16, 23.21s/it]\n",
      "6%|▌         | 42/763 [16:16<4:38:36, 23.19s/it]\n",
      "6%|▌         | 43/763 [16:38<4:35:43, 22.98s/it]\n",
      "6%|▌         | 44/763 [17:01<4:35:07, 22.96s/it]\n",
      "6%|▌         | 45/763 [17:24<4:35:07, 22.99s/it]\n",
      "6%|▌         | 46/763 [17:47<4:34:31, 22.97s/it]\n",
      "6%|▌         | 47/763 [18:10<4:33:11, 22.89s/it]\n",
      "6%|▋         | 48/763 [18:32<4:32:19, 22.85s/it]\n",
      "6%|▋         | 49/763 [18:55<4:31:55, 22.85s/it]\n",
      "7%|▋         | 50/763 [19:19<4:32:56, 22.97s/it]\n",
      "7%|▋         | 51/763 [19:42<4:35:23, 23.21s/it]\n",
      "7%|▋         | 52/763 [20:05<4:34:36, 23.17s/it]\n",
      "7%|▋         | 53/763 [20:29<4:34:02, 23.16s/it]\n",
      "7%|▋         | 54/763 [20:52<4:33:49, 23.17s/it]\n",
      "7%|▋         | 55/763 [21:15<4:32:16, 23.07s/it]\n",
      "7%|▋         | 56/763 [21:38<4:31:26, 23.04s/it]\n",
      "7%|▋         | 57/763 [22:00<4:28:40, 22.83s/it]\n",
      "8%|▊         | 58/763 [22:23<4:28:13, 22.83s/it]\n",
      "8%|▊         | 59/763 [22:45<4:26:31, 22.72s/it]\n",
      "8%|▊         | 60/763 [23:08<4:24:58, 22.62s/it]\n",
      "8%|▊         | 61/763 [23:30<4:23:57, 22.56s/it]\n",
      "8%|▊         | 62/763 [23:52<4:22:57, 22.51s/it]\n",
      "8%|▊         | 63/763 [24:15<4:24:37, 22.68s/it]\n",
      "8%|▊         | 64/763 [24:38<4:24:51, 22.73s/it]\n",
      "9%|▊         | 65/763 [25:02<4:28:34, 23.09s/it]\n",
      "9%|▊         | 66/763 [25:25<4:26:26, 22.94s/it]\n",
      "9%|▉         | 67/763 [25:49<4:28:44, 23.17s/it]\n",
      "9%|▉         | 68/763 [26:11<4:27:37, 23.10s/it]\n",
      "9%|▉         | 69/763 [26:34<4:26:39, 23.05s/it]\n",
      "9%|▉         | 70/763 [26:57<4:23:50, 22.84s/it]\n",
      "9%|▉         | 71/763 [27:21<4:28:47, 23.31s/it]\n",
      "9%|▉         | 72/763 [27:45<4:28:37, 23.32s/it]\n",
      "10%|▉         | 73/763 [28:07<4:25:34, 23.09s/it]\n",
      "10%|▉         | 74/763 [28:30<4:23:18, 22.93s/it]\n",
      "10%|▉         | 75/763 [28:53<4:24:49, 23.09s/it]\n",
      "10%|▉         | 76/763 [29:17<4:25:44, 23.21s/it]\n",
      "10%|█         | 77/763 [29:41<4:28:04, 23.45s/it]\n",
      "10%|█         | 78/763 [30:03<4:24:50, 23.20s/it]\n",
      "10%|█         | 79/763 [30:26<4:22:59, 23.07s/it]\n",
      "10%|█         | 80/763 [30:49<4:23:45, 23.17s/it]\n",
      "11%|█         | 81/763 [31:13<4:23:40, 23.20s/it]\n",
      "11%|█         | 82/763 [31:35<4:19:01, 22.82s/it]\n",
      "11%|█         | 83/763 [31:58<4:20:30, 22.99s/it]\n",
      "11%|█         | 84/763 [32:22<4:22:47, 23.22s/it]\n",
      "11%|█         | 85/763 [32:44<4:19:53, 23.00s/it]\n",
      "11%|█▏        | 86/763 [33:07<4:19:46, 23.02s/it]\n",
      "11%|█▏        | 87/763 [33:30<4:17:45, 22.88s/it]\n",
      "12%|█▏        | 88/763 [33:53<4:19:46, 23.09s/it]\n",
      "12%|█▏        | 89/763 [34:17<4:21:52, 23.31s/it]\n",
      "12%|█▏        | 90/763 [34:40<4:20:15, 23.20s/it]\n",
      "12%|█▏        | 91/763 [35:03<4:19:06, 23.14s/it]\n",
      "12%|█▏        | 92/763 [35:26<4:18:17, 23.10s/it]\n",
      "12%|█▏        | 93/763 [35:49<4:17:25, 23.05s/it]\n",
      "12%|█▏        | 94/763 [36:12<4:16:48, 23.03s/it]\n",
      "12%|█▏        | 95/763 [36:35<4:15:22, 22.94s/it]\n",
      "13%|█▎        | 96/763 [36:58<4:15:14, 22.96s/it]\n",
      "13%|█▎        | 97/763 [37:20<4:13:17, 22.82s/it]\n",
      "13%|█▎        | 98/763 [37:44<4:15:00, 23.01s/it]\n",
      "13%|█▎        | 99/763 [38:07<4:16:19, 23.16s/it]\n",
      "13%|█▎        | 100/763 [38:31<4:18:24, 23.39s/it]\n",
      "13%|█▎        | 100/763 [38:55<4:18:05, 23.36s/it]\n",
      "******epoch=2: train_ppl=tensor(1.2621, device='cuda:0') train_loss=tensor(0.2328, device='cuda:0')******\n",
      "0%|          | 0/38 [00:00<?, ?it/s]\n",
      "3%|▎         | 1/38 [00:03<02:13,  3.62s/it]\n",
      "5%|▌         | 2/38 [00:06<01:55,  3.21s/it]\n",
      "8%|▊         | 3/38 [00:09<01:47,  3.08s/it]\n",
      "11%|█         | 4/38 [00:12<01:42,  3.02s/it]\n",
      "13%|█▎        | 5/38 [00:15<01:38,  2.98s/it]\n",
      "16%|█▌        | 6/38 [00:18<01:34,  2.96s/it]\n",
      "18%|█▊        | 7/38 [00:21<01:31,  2.95s/it]\n",
      "21%|██        | 8/38 [00:24<01:28,  2.94s/it]\n",
      "24%|██▎       | 9/38 [00:26<01:24,  2.93s/it]\n",
      "26%|██▋       | 10/38 [00:29<01:21,  2.93s/it]\n",
      "29%|██▉       | 11/38 [00:32<01:18,  2.92s/it]\n",
      "32%|███▏      | 12/38 [00:35<01:16,  2.92s/it]\n",
      "34%|███▍      | 13/38 [00:38<01:13,  2.93s/it]\n",
      "37%|███▋      | 14/38 [00:41<01:10,  2.92s/it]\n",
      "39%|███▉      | 15/38 [00:44<01:07,  2.92s/it]\n",
      "42%|████▏     | 16/38 [00:47<01:04,  2.92s/it]\n",
      "45%|████▍     | 17/38 [00:50<01:01,  2.92s/it]\n",
      "47%|████▋     | 18/38 [00:53<00:58,  2.92s/it]\n",
      "50%|█████     | 19/38 [00:56<00:55,  2.92s/it]\n",
      "53%|█████▎    | 20/38 [00:59<00:52,  2.92s/it]\n",
      "55%|█████▌    | 21/38 [01:02<00:49,  2.92s/it]\n",
      "58%|█████▊    | 22/38 [01:04<00:46,  2.92s/it]\n",
      "61%|██████    | 23/38 [01:07<00:43,  2.92s/it]\n",
      "63%|██████▎   | 24/38 [01:10<00:40,  2.92s/it]\n",
      "66%|██████▌   | 25/38 [01:13<00:37,  2.92s/it]\n",
      "68%|██████▊   | 26/38 [01:16<00:35,  2.92s/it]\n",
      "71%|███████   | 27/38 [01:19<00:32,  2.92s/it]\n",
      "74%|███████▎  | 28/38 [01:22<00:29,  2.92s/it]\n",
      "76%|███████▋  | 29/38 [01:25<00:26,  2.92s/it]\n",
      "79%|███████▉  | 30/38 [01:28<00:23,  2.92s/it]\n",
      "82%|████████▏ | 31/38 [01:31<00:20,  2.92s/it]\n",
      "84%|████████▍ | 32/38 [01:34<00:17,  2.92s/it]\n",
      "87%|████████▋ | 33/38 [01:37<00:14,  2.92s/it]\n",
      "89%|████████▉ | 34/38 [01:40<00:11,  2.92s/it]\n",
      "92%|█████████▏| 35/38 [01:42<00:08,  2.92s/it]\n",
      "95%|█████████▍| 36/38 [01:45<00:05,  2.91s/it]\n",
      "97%|█████████▋| 37/38 [01:48<00:02,  2.91s/it]\n",
      "100%|██████████| 38/38 [01:51<00:00,  2.92s/it]\n",
      "100%|██████████| 38/38 [01:51<00:00,  2.94s/it]\n",
      "*******epoch=2: eval_ppl=tensor(3.6314, device='cuda:0') eval_loss=tensor(1.2896, device='cuda:0')*******\n",
      "Training done!\n",
      "2023-09-06 21:04:37,569 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-09-06 21:04:37,569 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-09-06 21:04:37,569 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2023-09-06 21:05:17 Uploading - Uploading generated training model\n",
      "2023-09-06 21:05:17 Completed - Resource retained for reuse\n",
      "Training seconds: 7623\n",
      "Billable seconds: 7623\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'train': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warm pool cluster if no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(huggingface_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
